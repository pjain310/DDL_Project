{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../understanding_cloud_organization/train.csv\")\n",
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = [\"Fish\",\"Flower\",\"Gravel\",\"Sugar\"]\n",
    "\n",
    "train_dict = {}\n",
    "train_class_dict = {}\n",
    "for idx, row in train_df.iterrows():\n",
    "    image_filename = row.Image_Label.split(\"_\")[0]\n",
    "    class_name = row.Image_Label.split(\"_\")[1]\n",
    "    class_id = category_list.index(class_name)\n",
    "    if train_dict.get(image_filename):\n",
    "        train_dict[image_filename].append(row.EncodedPixels)\n",
    "        train_class_dict[image_filename].append(class_id)\n",
    "    else:\n",
    "        train_dict[image_filename] = [row.EncodedPixels]\n",
    "        train_class_dict[image_filename] = [class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"image_id\",\"EncodedPixels\",\"CategoryId\",\"Width\",\"Height\"])\n",
    "for key, value in train_dict.items():\n",
    "    img = Image.open(\"../understanding_cloud_organization/train_images/{}\".format(key))\n",
    "    width, height = img.width, img.height\n",
    "    df = df.append({\"image_id\": key, \"EncodedPixels\": value, \"CategoryId\": train_class_dict[key], \"Width\": width, \"Height\": height},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../kaggle/input/')\n",
    "ROOT_DIR = \"../../working\"\n",
    "\n",
    "NUM_CATS = len(category_list)\n",
    "IMAGE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Mask_RCNN' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://www.github.com/matterport/Mask_RCNN.git\n",
    "os.chdir('Mask_RCNN')\n",
    "\n",
    "!rm -rf .git # to prevent an error when the kernel is committed\n",
    "!rm -rf images assets # to prevent displaying images at the bottom of a kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(ROOT_DIR+'/Mask_RCNN')\n",
    "from mrcnn.config import Config\n",
    "\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 priyankmadria  staff   246M Nov 25  2017 mask_rcnn_coco.h5\r\n"
     ]
    }
   ],
   "source": [
    "!wget --quiet https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
    "!ls -lh mask_rcnn_coco.h5\n",
    "\n",
    "COCO_WEIGHTS_PATH = 'mask_rcnn_coco.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     4\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 4\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                17\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              none\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           cloud\n",
      "NUM_CLASSES                    5\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                4500\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               500\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class CloudConfig(Config):\n",
    "    NAME = \"cloud\"\n",
    "    NUM_CLASSES = NUM_CATS + 1 # +1 for the background class\n",
    "    \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4 #That is the maximum with the memory available on kernels\n",
    "    \n",
    "    BACKBONE = 'resnet50'\n",
    "    \n",
    "    IMAGE_MIN_DIM = IMAGE_SIZE\n",
    "    IMAGE_MAX_DIM = IMAGE_SIZE    \n",
    "    IMAGE_RESIZE_MODE = 'none'\n",
    "    \n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n",
    "    \n",
    "    # STEPS_PER_EPOCH should be the number of instances \n",
    "    # divided by (GPU_COUNT*IMAGES_PER_GPU), and so should VALIDATION_STEPS;\n",
    "    # however, due to the time limit, I set them so that this kernel can be run in 9 hours\n",
    "    STEPS_PER_EPOCH = 4500\n",
    "    VALIDATION_STEPS = 500\n",
    "    \n",
    "config = CloudConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)  \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDataset(utils.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        # Add classes\n",
    "        for i, name in enumerate(category_list):\n",
    "            self.add_class(\"cloud\", i+1, name)\n",
    "        \n",
    "        # Add images \n",
    "        for i, row in df.iterrows():\n",
    "            self.add_image(\"cloud\", \n",
    "                           image_id=row.name, \n",
    "                           path='../understanding_cloud_organization/train_images/'+str(row.image_id), \n",
    "                           labels=row['CategoryId'],\n",
    "                           annotations=row['EncodedPixels'], \n",
    "                           height=row['Height'], width=row['Width'])\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path'], [category_list[int(x)] for x in info['labels']]\n",
    "    \n",
    "    def load_image(self, image_id):\n",
    "        return resize_image(self.image_info[image_id]['path'])\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "                \n",
    "        mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE, len(info['annotations'])), dtype=np.uint8)\n",
    "        labels = []\n",
    "        \n",
    "        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n",
    "            sub_mask = np.full(info['height']*info['width'], 0, dtype=np.uint8)\n",
    "            annotation = [int(x) for x in annotation.split(' ')]\n",
    "            \n",
    "            for i, start_pixel in enumerate(annotation[::2]):\n",
    "                sub_mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n",
    "\n",
    "            sub_mask = sub_mask.reshape((info['height'], info['width']), order='F')\n",
    "            sub_mask = cv2.resize(sub_mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            mask[:, :, m] = sub_mask\n",
    "            labels.append(int(label)+1)\n",
    "            \n",
    "        return mask, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('../understanding_cloud_organization/train_images/1cd4af5.jpg', ['Flower', 'Gravel', 'Sugar'])\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.1) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0969d550af7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mvisualize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_top_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cf764e89cf27>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(self, image_id)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresize_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-74d2aba99f76>\u001b[0m in \u001b[0;36mresize_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresize_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.1) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "training_percentage = 0.9\n",
    "\n",
    "training_set_size = int(training_percentage*len(df))\n",
    "validation_set_size = int((1-training_percentage)*len(df))\n",
    "\n",
    "train_dataset = CloudDataset(df[:training_set_size])\n",
    "train_dataset.prepare()\n",
    "\n",
    "valid_dataset = CloudDataset(df[training_set_size:training_set_size+validation_set_size])\n",
    "valid_dataset.prepare()\n",
    "\n",
    "for i in range(5):\n",
    "    image_id = random.choice(train_dataset.image_ids)\n",
    "    print(train_dataset.image_reference(image_id))\n",
    "    \n",
    "    image = train_dataset.load_image(image_id)\n",
    "    mask, class_ids = train_dataset.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, train_dataset.class_names, limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "EPOCHS = [3,9]\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Flipud(0.5)\n",
    "], random_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR)\n",
    "\n",
    "model.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\n",
    "    'mrcnn_class_logits', 'mrcnn_bbox_fc', 'mrcnn_bbox', 'mrcnn_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(train_dataset, valid_dataset,\n",
    "            learning_rate=LR*2,\n",
    "            epochs=EPOCHS[0],\n",
    "            layers='heads',\n",
    "            augmentation=None)\n",
    "\n",
    "history = model.keras_model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(train_dataset, valid_dataset,\n",
    "            learning_rate=LR,\n",
    "            epochs=EPOCHS[1],\n",
    "            layers='all',\n",
    "            augmentation=augmentation)\n",
    "\n",
    "new_history = model.keras_model.history.history\n",
    "for k in new_history: history[k] = history[k] + new_history[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(EPOCHS[-1])\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(epochs, history['loss'], label=\"train loss\")\n",
    "plt.plot(epochs, history['val_loss'], label=\"valid loss\")\n",
    "plt.legend()\n",
    "plt.subplot(132)\n",
    "plt.plot(epochs, history['mrcnn_class_loss'], label=\"train class loss\")\n",
    "plt.plot(epochs, history['val_mrcnn_class_loss'], label=\"valid class loss\")\n",
    "plt.legend()\n",
    "plt.subplot(133)\n",
    "plt.plot(epochs, history['mrcnn_mask_loss'], label=\"train mask loss\")\n",
    "plt.plot(epochs, history['val_mrcnn_mask_loss'], label=\"valid mask loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = np.argmin(history[\"val_loss\"]) + 1\n",
    "print(\"Best epoch: \", best_epoch)\n",
    "print(\"Valid loss: \", history[\"val_loss\"][best_epoch-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(CloudConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "model = modellib.MaskRCNN(mode='inference', \n",
    "                          config=inference_config,\n",
    "                          model_dir=ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_list = glob.glob(f'../../working/cloud*/mask_rcnn_cloud_{best_epoch:04d}.h5')\n",
    "model_path = glob_list[0] if glob_list else ''\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix overlapping masks\n",
    "def refine_masks(masks, rois):\n",
    "    areas = np.sum(masks.reshape(-1, masks.shape[-1]), axis=0)\n",
    "    mask_index = np.argsort(areas)\n",
    "    union_mask = np.zeros(masks.shape[:-1], dtype=bool)\n",
    "    for m in mask_index:\n",
    "        masks[:, :, m] = np.logical_and(masks[:, :, m], np.logical_not(union_mask))\n",
    "        union_mask = np.logical_or(masks[:, :, m], union_mask)\n",
    "    for m in range(masks.shape[-1]):\n",
    "        mask_pos = np.where(masks[:, :, m]==True)\n",
    "        if np.any(mask_pos):\n",
    "            y1, x1 = np.min(mask_pos, axis=1)\n",
    "            y2, x2 = np.max(mask_pos, axis=1)\n",
    "            rois[m, :] = [y1, x1, y2, x2]\n",
    "    return masks, rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(\"../../input/understanding_cloud_organization/sample_submission.csv\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns=[\"image_id\",\"EncodedPixels\",\"CategoryId\"])\n",
    "for idx,row in sample_df.iterrows():\n",
    "    image_filename = row.Image_Label.split(\"_\")[0]\n",
    "    test_df = test_df.append({\"image_id\": image_filename},ignore_index=True)\n",
    "test_df = test_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    image_id = test_df.sample()[\"image_id\"].values[0]\n",
    "    image_path = str('../../input/understanding_cloud_organization/test_images/'+image_id)\n",
    "    print(image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    result = model.detect([resize_image(image_path)])\n",
    "    r = result[0]\n",
    "    \n",
    "    if r['masks'].size > 0:\n",
    "        masks = np.zeros((img.shape[0], img.shape[1], r['masks'].shape[-1]), dtype=np.uint8)\n",
    "        for m in range(r['masks'].shape[-1]):\n",
    "            masks[:, :, m] = cv2.resize(r['masks'][:, :, m].astype('uint8'), \n",
    "                                        (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        y_scale = img.shape[0]/IMAGE_SIZE\n",
    "        x_scale = img.shape[1]/IMAGE_SIZE\n",
    "        rois = (r['rois'] * [y_scale, x_scale, y_scale, x_scale]).astype(int)\n",
    "        \n",
    "        masks, rois = refine_masks(masks, rois)\n",
    "    else:\n",
    "        masks, rois = r['masks'], r['rois']\n",
    "        \n",
    "    visualize.display_instances(img, rois, masks, r['class_ids'], \n",
    "                                ['bg']+category_list, r['scores'],\n",
    "                                title=image_id, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return ' '.join([str(x) for x in run_lengths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = sample_df.copy()\n",
    "submission_df[\"EncodedPixels\"] = \"\"\n",
    "with tqdm(total=len(test_df)) as pbar:\n",
    "    for i,row in test_df.iterrows():\n",
    "        pbar.update(1)\n",
    "        image_id = row[\"image_id\"]\n",
    "        image_path = str('../input/understanding_cloud_organization/test_images/'+image_id)\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        result = model.detect([resize_image(image_path)])\n",
    "        r = result[0]\n",
    "\n",
    "        if r['masks'].size > 0:\n",
    "            masks = np.zeros((img.shape[0], img.shape[1], r['masks'].shape[-1]), dtype=np.uint8)\n",
    "            for m in range(r['masks'].shape[-1]):\n",
    "                masks[:, :, m] = cv2.resize(r['masks'][:, :, m].astype('uint8'), \n",
    "                                            (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            y_scale = img.shape[0]/IMAGE_SIZE\n",
    "            x_scale = img.shape[1]/IMAGE_SIZE\n",
    "            rois = (r['rois'] * [y_scale, x_scale, y_scale, x_scale]).astype(int)\n",
    "            masks, rois, class_ids = r['masks'], r['rois'], r['class_ids']\n",
    "\n",
    "            #The following piece of code is creating rectangular masks from\n",
    "            # the ROIs instead of using the masks drawn by the MaskRCNN.\n",
    "            # It also removes any missing area from the imagery from the predicted masks.\n",
    "            # Everything is added directly to the submission dataframe.\n",
    "            rectangular_masks = []\n",
    "            mask_dict = {\"Fish\":[],\"Flower\":[],\"Gravel\":[],\"Sugar\":[]}\n",
    "            for roi, class_id in zip(rois, class_ids):\n",
    "                rectangular_mask = np.zeros((512,512))\n",
    "                rectangular_mask[roi[0]:roi[2], roi[1]:roi[3]] = 255\n",
    "                img = cv2.resize(img, dsize=(512,512), interpolation = cv2.INTER_LINEAR)\n",
    "                cropped_img = img[roi[0]:roi[2], roi[1]:roi[3]]\n",
    "                \n",
    "                kernel = np.ones((5,5),np.uint8)\n",
    "                missing_data = np.where(cropped_img[:,:,0]==0,255,0).astype('uint8')\n",
    "                contour_mask = np.zeros(missing_data.shape)\n",
    "                opening = cv2.morphologyEx(missing_data.astype('uint8'), cv2.MORPH_OPEN, kernel)\n",
    "                contours= cv2.findContours(opening,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                if len(contours[0])>0:\n",
    "                    largest_contour = max(contours[0], key = cv2.contourArea)\n",
    "                    cv2.fillPoly(contour_mask, pts =[largest_contour], color=(255))\n",
    "                    kernel = np.ones((5,5),np.uint8)\n",
    "                    opening = cv2.morphologyEx(contour_mask, cv2.MORPH_OPEN, kernel)\n",
    "                    fixed_mask = np.where(opening[:,:]==255,0,255)\n",
    "                    rectangular_mask[roi[0]:roi[2], roi[1]:roi[3]] = fixed_mask.copy()\n",
    "                    \n",
    "                if mask_dict[category_list[class_id-1]]==[]:\n",
    "                    mask_dict[category_list[class_id-1]] = rectangular_mask\n",
    "                else:\n",
    "                    previous_mask = mask_dict[category_list[class_id-1]].copy()\n",
    "                    #prevents a bug where the mask is in int64\n",
    "                    previous_mask = previous_mask.astype('float64')\n",
    "                    boolean_mask = np.ma.mask_or(previous_mask, rectangular_mask)\n",
    "                    merged_mask = np.where(boolean_mask, 255, 0)\n",
    "                    mask_dict[category_list[class_id-1]] = merged_mask\n",
    "\n",
    "            \n",
    "            #Going through the masks per category and create a md mask in RLE\n",
    "            for cloud_category in mask_dict.keys():\n",
    "                if mask_dict[cloud_category]!=[]:\n",
    "                    #resizing for submission\n",
    "                    resized_mask = cv2.resize((mask_dict[cloud_category]/255).astype('uint8'), dsize=(525,350), interpolation = cv2.INTER_LINEAR)\n",
    "                    rle_str = rle_encoding(resized_mask)\n",
    "                    image_label = \"{}_{}\".format(image_id,cloud_category)\n",
    "                    submission_df.loc[submission_df['Image_Label']==image_label,'EncodedPixels'] = rle_str\n",
    "        else:\n",
    "            masks, rois = r['masks'], r['rois']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.query(\"EncodedPixels!=''\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"../../working/submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
